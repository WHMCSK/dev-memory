# LLM QA

一、基座模型预训练的数据量和模型参数量的关系

数据量：模型参数量，DeepMind提出1:1.75，OpenAI提出的是1:20

二、预训练的语料从何而来

1. 找开源数据集
2. 通过爬虫在网上爬取，然后自己洗数据

三、做预训练需要什么样的硬件配置

1. 没有2块A100起基本上做不了
2. 比较大规模的预训练，至少需要32张A100，或64张A100
3. 单卡只能用来训练1B参数规模的模型

四、微调需要什么样的硬件配置

1. 大模型全量微调，需要具备4张A100的算力

五、双卡GPU，硬件升级路线？

3090 -> A100 40G -> A100 80G

六、现在做大模型预训练，语料一般多大

1. 最早大家都是用100B、200B，GPT3也是用的200B语料
2. 最近大家都用更大的，比如Lamma用的2T的语料，这种用64卡、128卡，怎么也得训一个月

七、我们行业的数据没有被包含到开源模型中，如何做出一个我们行业的模型？

这里要说Continues Pretraining（二次做训练）或者Pretraining。二次训练也就是做Instruction tuning，如果Instruct足够多，就不用碰预训练，只需要做Instruction tuneing就可以了。因为开源的这些预训练模型已经具备了足够的知识，再进行Instruction tuneing，其实是为了让大模型把这些知识更好的表达出来，也就是说人话。如果Instruction不够多，或者不够好，可以把Instruction放到预训练里面做

八、指令微调大概要用多少数据

1. Aplaca大概5千条
2. Vicuna用了几万条
3. 最少得有几千条，如果制作分类、抽取大概8千条以上就有非常好的效果

九、指令微调的数据有必要清晰吗？如何清洗？

1. 非常重要，SFT的数据质量是非常重要的一个指标，你可能需要的训练数据并不需要特别多，但是一定要足够的干净。
2. SFT数据如何清洗呢？一般都是人看、或者从ChatGPT套数据。SFT数据现在都是人写的，除了人写，就是ChatGpt去套。

十、做SFT训练大概需要什么样的GPU资源？

1. 一般用4卡到8卡比较合理，也是相对Basic的门槛
2. 一张两张卡也能把模型放进去，但是要多少时间就不确定了

十一、领域知识是在继续预训练阶段灌入，还是在SFT阶段去灌入？

这取决于数据是Instruction的形式还是无监督文本的形式，如果是“无监督文本的形式”，那就还是用继续预训练去做

十二、SFT的时候是对整个模型的权重进行微调吗，还是其中几层？

这种首先了解一下Lora，Lora是对每个层后面加层，或者说加适配器，原来的模型参数是Freeze的，没有对原来的模型进行调整。具体是做微调还是做全调，看算力，如果算力足够就两种都做，看那种结果更好，如果算力不够就用Lora就可以了。通过一般经验，全量微调比Lora微调效果会更好一些。不过在做全调的时候注意不能调太多的apok，不然模型以前有的一些能力也许会忘掉了。

十三、SFT的训练代码也是做文字接龙吗？与预训练代码有什么区别吗？

本质上来讲没有区别，都是做Next Token的Prediction。

十四、大模型有记忆能力吗？

大模型本身并没有记忆能力，传统模型的权重是固定的。但是现在的大模型都和向量数据库结合，可以具备长期记忆能力。向量数据库就像是AI的”海马体“结合了向量数据库的AI大模型越用越聪明、越用越迅捷。其做法是历史问答可以当作新的语料进入向量数据库永久储存。甚至当用户提出了重复、相似的提问，向量数据库会直接给出缓存答案。而随着向量数据库不断进化，这种记忆能力会越发强大。

十五、向量在AI大模型中是个什么角色？

向量是AI大模型理解现实世界的钥匙，当人类想到向量可以让计算机理解复杂的图像、文字、音频等信息时，我们似乎就找到了AI理解现实世界的钥匙。

十六、为什么不使用Gpt系列模型？
1. 存在网络限制
2. 数据要上传，老美的平台更加不可信
3. 按照token付费，其实在需求量大的情况下，费用很贵，成本控制很难
4. Gpt更像是个黑盒子，我们只能按照他设定的规则调用和使用，灵活性和扩展性方面不如本地部署的开源大模型
5. 由于以上几点，尽管开源大模型不如在线大模型，但是仍然在很多场景更加需要本地部署这些开源大模型。

十七、为什么不能用4090训练大模型

我们要考虑到训练过程，首先几个T的数据，我们要把它分发到不同的Gpu上，这个叫数据并行; 一个大模型的参数很难在一个Gpu上保存下来所以我们需要把大模型的不同层分发到不同的Gpu上做一个串联，这叫做流水线并行; 大模型的训练Transformer一般都是多头（多头注意力机制Multi-head Attention）的，这就涉及到一些张量并行(Tensor并行)。 数据并行，流水线并行，张量并行整体形成了大模型数据层、模型层内、模型层间的关系，这种关系通常涉及到参数的存储、GPU之间的通信计算，在这种情况下，内存带宽、通信延迟、通信带宽就显得尤为重要。

十八、Transformer为何使用多头注意力机制？（为什么不使用一个头）

[为什么Transformer 需要进行 Multi-head Attention？](https://www.zhihu.com/question/341222779)
 注解：简单回答就是，多头保证了transformer可以关注到不同子空间的信息，捕捉到更加丰富的特征信息。 


Instruction tuning stage

